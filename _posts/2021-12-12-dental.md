---
layout: post
title: Detection and Classification of Dental Panoramics with Mask R-CNNs
subtitle: Automating the Analysis of Dental X-Rays.
author: Mehdi Amine
---

## Introduction

Please note that this post is still under construction, but I'm choosing to publish it while incrementally adding to it.

At the time in which this post is being written, I still consider this project my greatest work. I describe here its objectives, techniques, limitations and achievements. 

The scope of the project would require much more than one post to describe it in details. There are long and detailed research papers about projects that focused solely on teeth classification, or automatic detection of teeth decay, ... In fact, every objective achieved here is on its own an intricate research and development project.

What was achieved can be further appreciated considering it was done in a year, without provision of ground truth data, and by a single person who had just graduated with catastrophic marks (yours truly).

Besides the technicalities that led to the results, it also matters for me to present the personal circumstances and the challenges that surrounded the work. I believe this approach enriches the text and promotes the engagement of my reader. It is also more enjoyable for me as the writer. I hope my enjoyment writing becomes your enjoyment reading. 


## Objectives

Dentists keep records of the state and progress of each patient. One process through which these records are structured is the creation of dental charts. These charts are graphical representations of the 32 types of human permanent teeth labelled according to a specific nomenclature. In dental charts each tooth can be assigned with a set of attributes that are either pathologies or restorations. The nomenclature used for labelling teeth in this project is FDI.

Dental charts used to be done on paper. Nowadays several software options are available for dentists. Some of them include speech control to enable vocal dictation. Despite its advances, the process remains slow and very redundant, and several dentists have started to look for solutions to accelerate it.

Research targetting the application of computer vision on this problem has achieved considerable results. Different methods have been applied with great success and led to the release of powerful software to assist dentists. My employer was interested in the implementation of a competing solution. Having previously developed a software that enabled the creation of dental charts through typing or voice control, they felt they could do better. They recruited me for this purpose.

At the time of my recruitment I had nearly exhausted my hopes of ever finding employment. I had graduated from my master's with very poor grades, couldn't find visa sponsoring work abroad, and couldn't find open positions in my country. Because of this, I suffered from low self-esteem and frail mental health.

Perhaps from fear of disappointing more than from sincerety, I tested my own competences before proceding with the job interview. In other words, I evaluated myself to see if it was worth letting others evaluate me. My anxiety wouldn't settle otherwise.

I was successful in implementing a Resnet to classify dental images based on the presence of tooth decay. I presented this work to my employer and gained their approval. Previous lurkings on Kaggle made me aware of the availability of [the dataset](https://www.kaggle.com/pushkar34/teeth-dataset) towards which I owe my success in being able to secure my first job.

Despite this small success, my low self-esteem was still in charge. I kept asking my employer during the interview whether I will be assisted in this position. Whether dentists will provide the required data. These questions were answered with affirmation. Not long after, I realized I was the only person recruited to do the whole work. The whole work, including the annotation of data.

The objectives in their order of execution were:
1. Detecting and classifying the 32 types of permanent teeth.
2. Detecting and classifying attributes that a tooth can have:
    - Implants.
    - Crowns attached to implants. 
    - Bridges and their components (pillars and pontics).
    - Missing teeth, including cases in which orthodontic treatments extracted a tooth and closed the gap between its two adjacent teeth.
    - Endodontic treatments. Otherwise known as root canals.
    - Caries.
3. Processing the outputs of the first and second objectives in a cohesive description that associated attributes to their assigned tooth.

Unlike this list, the reality was unstructured. Initially the objectives were described in very broad terms, we did not know what would be feasible. They continued to grow in ambition and complexity relative to the results I was delivering. 

## Data Annotation

### On Becoming a Dentist

I was provided with a dataset of around 16000 dental X-rays. The dataset was limited to images of panoramic radiographs and did not contain any ground truth. My accelerated course on the field of dentistry was done during a 30 minute call with my employer's partner. By the end of the conversation, I was deemed ready for my first task: detecting and classifying teeth according to the FDI notation.

|![fdi](assets/dental_imgs/fdi.png)|
|:--:|
| [*FDI teeth numbering system.*](https://www.researchgate.net/figure/FDI-teeth-numbering-system-11-18right-upper-1-8-21-28left-upper-1-8-31-38left-lower_fig4_331569041) Chen, Hu & Zhang, Kailai & Lyu, Peijun & Li, Hong & Zhang, Ludan & Wu, Ji & Lee, Chin-Hui. (2019)|

I was under lots of pressure. Every tooth is unique. Human teeth differ in size and shape from one person to another. I could not define a single rule for which I did not find a plethora of exceptions. 

Not all canines are elongated and pointy. Molars are not always bigger than premolars. The four mandibular incisors all look the same more often than not... The position of a tooth is a nice indicator, but relative to what? Another tooth? That would not only shift the same problem to another tooth, but it would also fail to account for cases of missing teeth.

Part of my confusion was caused by a frame of mind that was trying to solve the problem. This is a problem for which solutions can only be approximated. With extensive research, a couple all-nighters, and barrels of coffee and tea, I tuned my human model to analyze dental x-rays. Part of the tuning was letting go of perfection. Expert dentists make frequent errors classifying a tooth from an image cropped out of its context. Teeth do not carry (pun totally unintended) inherent visual properties that are reliable enough to distinguish them with high precision. This is especially the case between teeth of the same families: the 12 molars, 8 premolars, 4 canines, 8 incisors. And between families that are adjacent to one another: molars with premolars, premolars with canines, and canines with incisors.

In other words, we can divide teeth classification into 3 levels of difficulty:
- **Level 1**: telling apart molars from canines, or premolars from incisors. This is evident. 
- **Level 2**: telling apart molars from premolars, or canines from incisors. This is much less evident. 
- **Level 3**: Telling apart one molar from another, or one incisor from another. This is in many cases impossible without relying on information other than shape and size.


The sensible approach would have been to confront my employer. I was told prior to my recruitment that I will be assisted and that the relevant data will be provided by dentists. But even under normal circumstances, I am too agreeable for my own good. 

On different occasions my decisions were made under wrong presumptions. This is unfortunately a recurring theme throughout the project. No one knew any better and my word was law, despite my repeated demands for supervision. I was actually treated like an expert, regardless of how much I expressed my desire to be questionned. Picking up dentist jargon and delivering my first results convinced my employer that I always knew what I was doing.

### Bounding Boxes and Polygons
Every published research I could find was relying on bounding boxes for teeth annotations. They made sense in terms of ease of use; dragging a rectangular frame around a tooth is quicker than drawing polygonal contours. Besides, image annotation tools, especially the free ones, are predominantly limited to bounding boxes. This goes in line with the fact that prior to the advance of segmentation models, object detection models did not require more than rectangular frames for the label input.

I was not convinced that this was a better approach compared to polygonal contours. The crowded space in dental x-rays conflicted with the straight rectangular nature of bounding boxes. Teeth are seldom straight from root to crown. Hence most bounding boxes end up including not only the target tooth, but also parts of its neighboors on both sides.

I could not hope to compete with large teams of dentists collaborating to annotate an abundant number of instances. I could however hope that a more precise annotation would compensate for the number difference. Quality over quantity. 

It turned out my hope was unfounded, and my understanding flawed. Segmentation with polygonal contours does not replace Object Detection with bounding boxes, it builds on top of it. In order to train the segmentation layers with polygons, bounding boxes need first to be provided to train the Region Proposal Network. So essentially, in my hopes to make things easier I was adding another layer of complexity. 

### Hasty
The tool I used for annotation is [Hasty.ai](https://hasty.ai/). My choice was limited to free software with segmentation capabilities. Hasty advertizes AI assistance. It tries to learn and suggests annotations based on previous ones. These suggestions are limited in their number to what the free version allows. In my experience, the assistance feature was not reliable for dental X-Rays. Hasty as a tool however was extremely robust and it never failed me.

I used Hasty to manually label more than 14000 contours to create instances for the 38 classes (32 teeth and 6 attributes). If we consider that each contour had at least 7 points, the total number of points created is more than 98000. Each of them carefully placed to trace around the edges of each instance.

Every time I finished a batch of a 100 instances, I used the tool to generate a JSON file of the annotation in a COCO dataset format. The main fields in the JSON file are:
* **Categories**: a list of dictionaries containing the ```id```, ```name```, and ```supercategory``` of the classes. The id is a unique integer that starts with 1. It is important to note that the COCO format reserves the 0 id for the category of the background. Example category: ```{"id": 1, "name": "11",
"supercategory": "object"}```
* **Images**: a list of dictionaries containing image metadata. The main fields required are the ```id```, ```width```, ```height```, and ```file_name```. The format provides other fields that I consider optional as they are not consumed in neither training nor inference. Example image: ```{"id": 1, "width": 900, "height": 478, "file_name": "Panoramic_Radio_1.jpg", "license": null, "flickr_url": "", "coco_url": null, "date_captured": 
"2020-07-07T15:35:41Z"}```
* **Annotations**: a list of dictionaries containing annotation points, their label, and reference to their image. Each annotation has a unique ```id``` and points to its image through an ```image_id```. A ```segmentation``` field stores a list of lists of the (x, y) coordinates for each point in the polygon contour. It is a list of lists to account for situations in which parts of an object are seperated or occluded, thus multiple polygons are required to annotate the visible parts. The ```area``` field is the surface enclosed by the polygon(s) of the object. The ```bbox``` field is the bounding box or rectangular frame surrounding the polygon(s). The first two values in ```bbox``` are the x and y coordinates of the top left point in the box. The last two values are respectively the width and height of the box. It is important to keep in mind that not all models supporting the COCO format have as default setting for ```bbox```the width and height values. [Different modes are available](https://detectron2.readthedocs.io/en/latest/modules/structures.html?highlight=boxmode#detectron2.structures.BoxMode), including boxes with x and y coordinates for the top left point, followed by another pair of x and y for the bottom right point: [X_tl, Y_tl, X_br, Y_br]. Finally, the ```iscrowd``` field is 0 for annotations of one object, and 1 for annotations that group multiple objects. In this project, all annotations have ```iscrowd``` set to 0. Example annotation: ```{“id": 0, "image_id": 131, "category_id": 1, "segmentation":[[439, 78, 452, 78, 457, 82, 468, 108, 471, 118, 474, 127, 476, 135, 478, 146, 479, 171, 478, 181, 477, 188, 474, 193, 469, 194, 459, 193, 448, 195, 434, 195, 433, 191, 431, 179, 430, 159, 431, 112, 432, 96, 433, 90, 436, 82, 440, 78]], "area": 4731.0, "bbox": [430, 78, 49, 117], "iscrowd": 0}```


Hasty's assistive AI did not work, but the idea sparked my interest. Training a model to help train a model was the "help me help you" relationship I strived to have with my system. I constructed the following pipeline: 
1. Manually annotate a batch of 100.
2. Extract the corresponding COCO JSON from Hasty.
3. If a previous JSON file exists, merge the new JSON file with the previous one.
3. Train and evaluate the model. 
4. If the model is getting better at segmenting than it is at classifying, proceed to 5. Otherwise go to 1.
5. Run inference on a batch of 100.
6. Convert the output to COCO JSON.
7. Feed Hasty with the batch images and their JSON.
8. Manually correct the wrong classifications and adjust the inaccurate segmentations. Go to 2.

This method worked well for teeth detections because the model learned to segment teeth much quicker than classifying them. Even at the stage in which the segmentation was not precise yet, manually adjusting an inaccurate polygon is much faster than creating one from scratch. The classification errors were also conveniently quick to correct from one FDI number to another.


## Results
In this section, I run inference with different panoramics to demonstrate the capabilities of the system. Let's begin with a simple panoramic: no missing teeth other than the four commonly missing wisdom teeth, all remaining teeth are intact. 

You can use the FDI chart for reference to evaluate the output. The number on the left of each bounding box is the inferred FDI label, the percentage is the probability associated with each output: or the certainty level of the model.

|![pano1](assets/dental_imgs/p1.jpg)|
|:--:|
|*X-ray image input using the Teeth Detection Model.*|
|:--:|
|![result1](assets/dental_imgs/r1.jpg)|
|:--:|
|*X-ray image with visualization of the Teeth Model output.*|

### API Demo
<p align="center"><iframe width="728" height="410" src="https://www.youtube.com/embed/OhbmKx8DehI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>

The contours around molars do not seperate between the roots, they include the space between them. The model has learned my laziness in annotating molars. It is common to find at least one error per image for teeth classification. These errors while common, are cases of the level 3 difficulty.

With the next image we visualize the segmentation of the following attributes: 
* endo: denotes root canals otherwise known as endodontic treatments.
* implant: denotes artificial roots on which crowns can be fixed.
* inter: denotes bridge intermediaries, sometimes called pontics. 
* pillar: denotes any tooth used to stabilize a bridge. 

|![pano2](assets/dental_imgs/p2.jpg)|
|:--:|
|*X-ray image input using the Attributes Detection Model.*|
|:--:|
|![result2](assets/dental_imgs/r2.jpg)|
|:--:|
|*X-ray Image with visualization of the Attributes Model output.*|

### API Demo
Along with the image above, the following demo uses a second image to show a case of crown detection. 

<p align="center"><iframe width="728" height="410" src="https://www.youtube.com/embed/BxDX_nWExAI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>

Annotation limitations prevented me from including more than one type of crowns. I restricted my annotations to crowns placed on implants. Sometimes the model detects crowns fixed on endodontic treatments showing that the visual properties have been learned and that the crowns I did not label are causing training conflicts. The model outputs higher crown probabilities when it can include part of the implant on which it is fixed. When detected, the other types of crowns, although correct, are usually filtered by the probability threshold that restricts the output to the most certain positives. 
